"""AI é©±åŠ¨çš„å®éªŒåˆ†ææ¨¡å—"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Dict, List, Optional, Any

from .agent_llm import AgentsLLM
from .ui import print_info, print_success, print_error, print_section_header, console


class ExperimentAnalyzer:
    """å®éªŒåˆ†æå™¨ - ä½¿ç”¨ AI åˆ†æå®éªŒç»“æœå¹¶æä¾›å»ºè®®"""
    
    def __init__(
        self,
        enable_ai: bool = True,
        llm_config: Optional[Dict[str, Any]] = None
    ):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            enable_ai: æ˜¯å¦å¯ç”¨ AI åˆ†æåŠŸèƒ½
            llm_config: LLM é…ç½®å­—å…¸ï¼ˆprovider, api_key, model, base_urlï¼‰
        """
        self.enable_ai = enable_ai
        self.llm: Optional[AgentsLLM] = None
        
        if enable_ai:
            try:
                # å¦‚æœæä¾›äº†é…ç½®ï¼Œä½¿ç”¨é…ç½®ï¼›å¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
                if llm_config:
                    self.llm = AgentsLLM(
                        provider=llm_config.get('llm_provider'),
                        api_key=llm_config.get('llm_api_key'),
                        base_url=llm_config.get('llm_base_url'),
                        model=llm_config.get('llm_model'),
                        temperature=0.7
                    )
                else:
                    self.llm = AgentsLLM(temperature=0.7)
                
                print_success(f"âœ“ AI åˆ†æå·²å¯ç”¨ (æä¾›å•†: {self.llm.provider}, æ¨¡å‹: {self.llm.model})")
            except Exception as e:
                print_error(f"âœ— æ— æ³•åˆå§‹åŒ– AI æ¨¡å‹: {e}")
                print_info("â„¹ å°†ä½¿ç”¨åŸºç¡€åˆ†æåŠŸèƒ½ï¼ˆä¸å« AIï¼‰")
                print_info("â„¹ æç¤º: è¿è¡Œ 'sciagent init' é…ç½® AI åŠŸèƒ½")
                self.enable_ai = False
    
    def analyze_run(
        self,
        run_record: Dict[str, Any],
        history: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """
        åˆ†æå•æ¬¡è¿è¡Œç»“æœ
        
        Args:
            run_record: è¿è¡Œè®°å½•
            history: å†å²è¿è¡Œè®°å½•ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åˆ†ææŠ¥å‘Šæ–‡æœ¬
        """
        if not self.enable_ai or not self.llm:
            return self._basic_analysis(run_record)
        
        try:
            return self._ai_analysis(run_record, history)
        except Exception as e:
            print_error(f"AI åˆ†æå¤±è´¥: {e}")
            print_info("ä½¿ç”¨åŸºç¡€åˆ†æ...")
            return self._basic_analysis(run_record)
    
    def _basic_analysis(self, run_record: Dict[str, Any]) -> str:
        """åŸºç¡€åˆ†æï¼ˆä¸ä½¿ç”¨ AIï¼‰"""
        analysis = []
        
        # æå–å…³é”®ä¿¡æ¯
        metrics = run_record.get("metrics", {})
        status = run_record.get("status", "unknown")
        
        analysis.append("## ğŸ“Š å®éªŒåˆ†æ\n")
        
        # çŠ¶æ€åˆ†æ
        if status == "completed":
            analysis.append("âœ… **è¿è¡ŒçŠ¶æ€**: æˆåŠŸå®Œæˆ\n")
        elif status == "failed":
            analysis.append("âŒ **è¿è¡ŒçŠ¶æ€**: è¿è¡Œå¤±è´¥\n")
        else:
            analysis.append(f"âš ï¸  **è¿è¡ŒçŠ¶æ€**: {status}\n")
        
        # æŒ‡æ ‡åˆ†æ
        if metrics:
            analysis.append("\n### æŒ‡æ ‡æ¦‚è§ˆ\n")
            for key, value in metrics.items():
                analysis.append(f"- **{key}**: {value}\n")
        
        # åŸºç¡€å»ºè®®
        analysis.append("\n### ğŸ’¡ å»ºè®®\n")
        analysis.append("1. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶äº†è§£è¯¦ç»†æ‰§è¡Œæƒ…å†µ\n")
        analysis.append("2. å¯¹æ¯”å†å²è¿è¡Œæ‰¾å‡ºæ€§èƒ½å·®å¼‚\n")
        analysis.append("3. è€ƒè™‘è°ƒæ•´è¶…å‚æ•°è¿›è¡Œä¼˜åŒ–\n")
        
        return "".join(analysis)
    
    def _ai_analysis(
        self,
        run_record: Dict[str, Any],
        history: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """AI é©±åŠ¨çš„æ·±åº¦åˆ†æ"""
        print_info("æ­£åœ¨ä½¿ç”¨ AI åˆ†æå®éªŒç»“æœ...")
        
        # æ„å»ºæç¤ºè¯
        prompt = self._build_analysis_prompt(run_record, history)
        
        messages = [
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœºå™¨å­¦ä¹ å®éªŒåˆ†æä¸“å®¶ã€‚"
                    "ä½ éœ€è¦åˆ†æå®éªŒç»“æœï¼Œæ‰¾å‡ºé—®é¢˜ï¼Œå¹¶æä¾›å…·ä½“å¯è¡Œçš„æ”¹è¿›å»ºè®®ã€‚"
                    "ä½ çš„å»ºè®®åº”è¯¥ï¼š\n"
                    "1. åŸºäºå®éªŒæ•°æ®ï¼Œæœ‰ç†æœ‰æ®\n"
                    "2. æä¾›å…·ä½“çš„å‚æ•°è°ƒæ•´å»ºè®®\n"
                    "3. æŒ‡å‡ºå¯èƒ½å­˜åœ¨çš„é—®é¢˜\n"
                    "4. ç»™å‡ºä¸‹ä¸€æ­¥å®éªŒæ–¹å‘\n"
                    "è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä½¿ç”¨ Markdown æ ¼å¼ã€‚"
                )
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
        
        # æµå¼è·å– AI å“åº”
        analysis_chunks = []
        console.print("\n[cyan]ğŸ¤– AI åˆ†æä¸­...[/cyan]\n")
        
        for chunk in self.llm.think(messages):
            analysis_chunks.append(chunk)
        
        console.print()
        
        return "".join(analysis_chunks)
    
    def _build_analysis_prompt(
        self,
        run_record: Dict[str, Any],
        history: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """æ„å»ºåˆ†ææç¤ºè¯"""
        prompt_parts = []
        
        prompt_parts.append("è¯·åˆ†æä»¥ä¸‹å®éªŒè¿è¡Œç»“æœï¼š\n\n")
        
        # åŸºæœ¬ä¿¡æ¯
        prompt_parts.append("## å®éªŒä¿¡æ¯\n")
        prompt_parts.append(f"- è¿è¡Œåç§°: {run_record.get('name', 'unnamed')}\n")
        prompt_parts.append(f"- è¿è¡ŒçŠ¶æ€: {run_record.get('status', 'unknown')}\n")
        prompt_parts.append(f"- å¼€å§‹æ—¶é—´: {run_record.get('start_time', 'N/A')}\n")
        
        if run_record.get('command'):
            prompt_parts.append(f"- æ‰§è¡Œå‘½ä»¤: {run_record['command']}\n")
        
        # é…ç½®ä¿¡æ¯
        if run_record.get('config'):
            prompt_parts.append("\n## é…ç½®å‚æ•°\n")
            prompt_parts.append("```json\n")
            prompt_parts.append(json.dumps(run_record['config'], indent=2, ensure_ascii=False))
            prompt_parts.append("\n```\n")
        
        # å…ƒæ•°æ®
        if run_record.get('metadata'):
            prompt_parts.append("\n## å…ƒæ•°æ®\n")
            for key, value in run_record['metadata'].items():
                prompt_parts.append(f"- {key}: {value}\n")
        
        # æŒ‡æ ‡
        if run_record.get('metrics'):
            prompt_parts.append("\n## å®éªŒæŒ‡æ ‡\n")
            for key, value in run_record['metrics'].items():
                prompt_parts.append(f"- {key}: {value}\n")
        
        # å†å²å¯¹æ¯”
        if history and len(history) > 0:
            prompt_parts.append("\n## å†å²å¯¹æ¯”\n")
            prompt_parts.append("æœ€è¿‘ 3 æ¬¡è¿è¡Œçš„ä¸»è¦æŒ‡æ ‡å¯¹æ¯”ï¼š\n\n")
            
            for i, hist_run in enumerate(history[-3:], 1):
                hist_metrics = hist_run.get('metrics', {})
                primary_metric = hist_run.get('primary_metric_value', 'N/A')
                prompt_parts.append(
                    f"{i}. {hist_run.get('name', 'unnamed')}: "
                    f"ä¸»è¦æŒ‡æ ‡={primary_metric}\n"
                )
        
        # åˆ†æè¦æ±‚
        prompt_parts.append("\n## è¯·æä¾›ä»¥ä¸‹åˆ†æï¼š\n")
        prompt_parts.append("1. **ç»“æœè¯„ä¼°**: è¯„ä»·æœ¬æ¬¡å®éªŒçš„è¡¨ç°å¦‚ä½•\n")
        prompt_parts.append("2. **é—®é¢˜è¯Šæ–­**: æŒ‡å‡ºå¯èƒ½å­˜åœ¨çš„é—®é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰\n")
        prompt_parts.append("3. **å‚æ•°è°ƒä¼˜å»ºè®®**: å…·ä½“è¯´æ˜å“ªäº›å‚æ•°åº”è¯¥å¦‚ä½•è°ƒæ•´\n")
        prompt_parts.append("4. **ä¸‹ä¸€æ­¥å®éªŒæ–¹å‘**: å»ºè®®æ¥ä¸‹æ¥åº”è¯¥å°è¯•ä»€ä¹ˆ\n")
        
        if history:
            prompt_parts.append("5. **ä¸å†å²å¯¹æ¯”**: ç›¸æ¯”å†å²è¿è¡Œæœ‰ä½•æ”¹è¿›æˆ–é€€æ­¥\n")
        
        return "".join(prompt_parts)
    
    def suggest_next_experiments(
        self,
        current_run: Dict[str, Any],
        num_suggestions: int = 3
    ) -> List[Dict[str, str]]:
        """
        å»ºè®®ä¸‹ä¸€æ­¥å®éªŒ
        
        Args:
            current_run: å½“å‰è¿è¡Œè®°å½•
            num_suggestions: å»ºè®®æ•°é‡
            
        Returns:
            å®éªŒå»ºè®®åˆ—è¡¨
        """
        if not self.enable_ai or not self.llm:
            return self._basic_suggestions(current_run, num_suggestions)
        
        try:
            return self._ai_suggestions(current_run, num_suggestions)
        except Exception as e:
            print_error(f"ç”Ÿæˆå»ºè®®å¤±è´¥: {e}")
            return self._basic_suggestions(current_run, num_suggestions)
    
    def _basic_suggestions(
        self,
        current_run: Dict[str, Any],
        num_suggestions: int
    ) -> List[Dict[str, str]]:
        """åŸºç¡€å»ºè®®ï¼ˆä¸ä½¿ç”¨ AIï¼‰"""
        suggestions = [
            {
                "title": "è°ƒæ•´å­¦ä¹ ç‡",
                "description": "å°è¯•é™ä½æˆ–æé«˜å­¦ä¹ ç‡ï¼Œè§‚å¯Ÿæ”¶æ•›é€Ÿåº¦çš„å˜åŒ–",
                "command_hint": "--lr 0.001 æˆ– --lr 0.0001"
            },
            {
                "title": "å¢åŠ è®­ç»ƒè½®æ•°",
                "description": "å¦‚æœæ¨¡å‹è¿˜åœ¨æ”¹è¿›ï¼Œå¯ä»¥å»¶é•¿è®­ç»ƒæ—¶é—´",
                "command_hint": "--epochs 200"
            },
            {
                "title": "è°ƒæ•´æ‰¹æ¬¡å¤§å°",
                "description": "æ›´å¤§çš„ batch size å¯èƒ½å¸¦æ¥æ›´ç¨³å®šçš„è®­ç»ƒ",
                "command_hint": "--batch-size 64"
            }
        ]
        
        return suggestions[:num_suggestions]
    
    def _ai_suggestions(
        self,
        current_run: Dict[str, Any],
        num_suggestions: int
    ) -> List[Dict[str, str]]:
        """AI ç”Ÿæˆçš„å®éªŒå»ºè®®"""
        print_info("æ­£åœ¨ç”Ÿæˆå®éªŒå»ºè®®...")
        
        prompt = f"""åŸºäºä»¥ä¸‹å®éªŒç»“æœï¼Œè¯·æä¾› {num_suggestions} ä¸ªå…·ä½“çš„ä¸‹ä¸€æ­¥å®éªŒå»ºè®®ï¼š

å®éªŒä¿¡æ¯ï¼š
- è¿è¡Œåç§°: {current_run.get('name', 'unnamed')}
- è¿è¡ŒçŠ¶æ€: {current_run.get('status', 'unknown')}
- æŒ‡æ ‡: {json.dumps(current_run.get('metrics', {}), ensure_ascii=False)}
- é…ç½®: {json.dumps(current_run.get('config', {}), ensure_ascii=False)}

è¯·ä»¥ JSON æ ¼å¼è¿”å›å»ºè®®åˆ—è¡¨ï¼Œæ¯ä¸ªå»ºè®®åŒ…å«ï¼š
- title: å»ºè®®æ ‡é¢˜
- description: è¯¦ç»†æè¿°
- command_hint: å‘½ä»¤è¡Œå‚æ•°æç¤º

ç¤ºä¾‹æ ¼å¼ï¼š
[
  {{
    "title": "è°ƒæ•´å­¦ä¹ ç‡",
    "description": "å½“å‰å­¦ä¹ ç‡å¯èƒ½è¿‡é«˜ï¼Œå»ºè®®é™ä½åˆ° 0.0001",
    "command_hint": "--lr 0.0001"
  }}
]

è¯·åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚"""
        
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªæœºå™¨å­¦ä¹ å®éªŒè®¾è®¡ä¸“å®¶ã€‚è¯·æ ¹æ®å®éªŒç»“æœæä¾›å…·ä½“å¯è¡Œçš„ä¸‹ä¸€æ­¥å®éªŒå»ºè®®ã€‚"
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
        
        response_text = "".join(self.llm.think(messages))
        
        # å°è¯•è§£æ JSON
        try:
            # æå– JSON éƒ¨åˆ†
            import re
            json_match = re.search(r'\[[\s\S]*\]', response_text)
            if json_match:
                suggestions = json.loads(json_match.group())
                return suggestions[:num_suggestions]
        except Exception as e:
            print_error(f"è§£æå»ºè®®å¤±è´¥: {e}")
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸºç¡€å»ºè®®
        return self._basic_suggestions(current_run, num_suggestions)


def analyze_run_from_file(
    run_record_path: Path,
    history_path: Optional[Path] = None,
    enable_ai: bool = True,
    config_path: Optional[Path] = None
) -> str:
    """
    ä»æ–‡ä»¶åˆ†æè¿è¡Œè®°å½•
    
    Args:
        run_record_path: è¿è¡Œè®°å½•æ–‡ä»¶è·¯å¾„
        history_path: å†å²è®°å½•æ–‡ä»¶è·¯å¾„
        enable_ai: æ˜¯å¦å¯ç”¨ AI
        config_path: SciAgent é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å« LLM é…ç½®ï¼‰
        
    Returns:
        åˆ†ææŠ¥å‘Š
    """
    # è¯»å–è¿è¡Œè®°å½•
    with open(run_record_path, 'r', encoding='utf-8') as f:
        run_record = json.load(f)
    
    # è¯»å–å†å²è®°å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    history = None
    if history_path and history_path.exists():
        with open(history_path, 'r', encoding='utf-8') as f:
            history_data = json.load(f)
            history = history_data.get('runs', [])
    
    # è¯»å– LLM é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    llm_config = None
    if config_path and config_path.exists():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                # æå– LLM ç›¸å…³é…ç½®
                if any(k.startswith('llm_') for k in config.keys()):
                    llm_config = {
                        k: v for k, v in config.items() 
                        if k.startswith('llm_') or k == 'enable_ai'
                    }
        except Exception:
            pass
    
    # åˆ›å»ºåˆ†æå™¨å¹¶åˆ†æ
    analyzer = ExperimentAnalyzer(enable_ai=enable_ai, llm_config=llm_config)
    analysis = analyzer.analyze_run(run_record, history)
    
    return analysis

