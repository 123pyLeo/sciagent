"""
stdout æ—¥å¿—è‡ªåŠ¨è§£ææ¨¡å—

è‡ªåŠ¨ä»è®­ç»ƒè¾“å‡ºä¸­æå–æŒ‡æ ‡ï¼Œæ— éœ€ç”¨æˆ·ä¿®æ”¹ä»»ä½•ä»£ç ã€‚
æ”¯æŒå¸¸è§çš„æ·±åº¦å­¦ä¹ æ¡†æ¶è¾“å‡ºæ ¼å¼ã€‚
"""

import re
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field


@dataclass
class ParsedMetrics:
    """è§£æåçš„æŒ‡æ ‡æ•°æ®"""
    # æœ€ç»ˆæŒ‡æ ‡ï¼ˆå–æœ€åä¸€æ¬¡å‡ºç°çš„å€¼ï¼‰
    final_metrics: Dict[str, float] = field(default_factory=dict)
    # å†å²æŒ‡æ ‡ï¼ˆç”¨äºè¿½è¸ªè¶‹åŠ¿ï¼‰
    history: Dict[str, List[float]] = field(default_factory=dict)
    # æ£€æµ‹åˆ°çš„ epoch æ•°
    epochs_detected: int = 0
    # æœ€ä½³æŒ‡æ ‡
    best_metrics: Dict[str, float] = field(default_factory=dict)
    # è®­ç»ƒé…ç½®ï¼ˆä»æ—¥å¿—ä¸­æå–ï¼‰
    config: Dict[str, Any] = field(default_factory=dict)


class LogParser:
    """è®­ç»ƒæ—¥å¿—è§£æå™¨"""
    
    # å¸¸è§çš„æŒ‡æ ‡åç§°å˜ä½“æ˜ å°„åˆ°æ ‡å‡†åç§°
    METRIC_ALIASES = {
        # Loss ç±»
        'loss': 'loss',
        'train_loss': 'train_loss',
        'val_loss': 'val_loss',
        'test_loss': 'test_loss',
        'validation_loss': 'val_loss',
        'training_loss': 'train_loss',
        # Accuracy ç±»
        'acc': 'accuracy',
        'accuracy': 'accuracy',
        'train_acc': 'train_accuracy',
        'val_acc': 'val_accuracy',
        'test_acc': 'test_accuracy',
        'train_accuracy': 'train_accuracy',
        'val_accuracy': 'val_accuracy',
        'test_accuracy': 'test_accuracy',
        'validation_accuracy': 'val_accuracy',
        # å…¶ä»–å¸¸è§æŒ‡æ ‡
        'f1': 'f1_score',
        'f1_score': 'f1_score',
        'f1-score': 'f1_score',
        'precision': 'precision',
        'recall': 'recall',
        'auc': 'auc',
        'auroc': 'auc',
        'mse': 'mse',
        'mae': 'mae',
        'rmse': 'rmse',
        'lr': 'learning_rate',
        'learning_rate': 'learning_rate',
    }
    
    # æŒ‡æ ‡æå–æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    PATTERNS = [
        # æ ¼å¼: "Loss: 0.1234" æˆ– "loss=0.1234" æˆ– "loss: 0.1234"
        r'(?P<name>[\w_]+)[\s]*[:=][\s]*(?P<value>[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?)',
        
        # æ ¼å¼: "Acc: 97.82%" æˆ– "accuracy: 97.82%"
        r'(?P<name>[\w_]+)[\s]*[:=][\s]*(?P<value>[-+]?\d*\.?\d+)[\s]*%',
        
        # æ ¼å¼: "è®­ç»ƒ - Loss: 0.1234, Acc: 92.34%"ï¼ˆä¸­æ–‡æ ‡ç­¾ï¼‰
        r'(?:è®­ç»ƒ|æµ‹è¯•|éªŒè¯)[\s]*[-â€“][\s]*(?P<pairs>(?:[\w_]+[\s]*[:=][\s]*[-+]?\d*\.?\d+%?[\s]*[,ï¼Œ]?[\s]*)+)',
        
        # æ ¼å¼: "Epoch 1/10" æˆ– "epoch: 1"
        r'[Ee]poch[\s]*(?:[:=][\s]*)?(?P<epoch>\d+)(?:/(?P<total>\d+))?',
        
        # æ ¼å¼: "[Epoch 5] loss: 0.123, acc: 0.95"
        r'\[?[Ee]poch[\s]*(?P<epoch>\d+)\]?[\s]*(?P<pairs>(?:[\w_]+[\s]*[:=][\s]*[-+]?\d*\.?\d+%?[\s]*[,ï¼Œ]?[\s]*)+)',
    ]
    
    # é…ç½®å‚æ•°æå–æ¨¡å¼
    CONFIG_PATTERNS = [
        # æ ¼å¼: "learning_rate: 0.001" æˆ– "lr=0.001"
        r'(?P<name>learning_rate|lr|batch_size|epochs?|num_epochs|hidden_dim|dropout|weight_decay)[\s]*[:=][\s]*(?P<value>[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?)',
    ]
    
    def __init__(self):
        self.compiled_patterns = [re.compile(p, re.IGNORECASE) for p in self.PATTERNS]
        self.compiled_config_patterns = [re.compile(p, re.IGNORECASE) for p in self.CONFIG_PATTERNS]
    
    def parse(self, log_content: str) -> ParsedMetrics:
        """
        è§£ææ—¥å¿—å†…å®¹ï¼Œæå–æŒ‡æ ‡
        
        Args:
            log_content: æ—¥å¿—æ–‡æœ¬å†…å®¹
            
        Returns:
            ParsedMetrics å¯¹è±¡
        """
        result = ParsedMetrics()
        
        # æŒ‰è¡Œå¤„ç†
        lines = log_content.split('\n')
        
        for line in lines:
            # è·³è¿‡è¿›åº¦æ¡è¡Œï¼ˆtqdm ç­‰ï¼‰
            if self._is_progress_bar(line):
                continue
            
            # æå–é…ç½®å‚æ•°
            self._extract_config(line, result)
            
            # æå– epoch ä¿¡æ¯
            self._extract_epoch(line, result)
            
            # æå–æŒ‡æ ‡
            self._extract_metrics(line, result)
            
            # æ£€æµ‹æœ€ä½³æŒ‡æ ‡
            self._extract_best_metrics(line, result)
        
        # ç”Ÿæˆæœ€ç»ˆæŒ‡æ ‡
        self._finalize_metrics(result)
        
        return result
    
    def _is_progress_bar(self, line: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºè¿›åº¦æ¡è¡Œ"""
        # tqdm è¿›åº¦æ¡ç‰¹å¾
        if '|' in line and ('it/s' in line or 'B/s' in line):
            return True
        # ç™¾åˆ†æ¯”è¿›åº¦æ¡
        if re.match(r'^\s*\d+%\|', line):
            return True
        return False
    
    def _extract_config(self, line: str, result: ParsedMetrics) -> None:
        """æå–é…ç½®å‚æ•°"""
        for pattern in self.compiled_config_patterns:
            for match in pattern.finditer(line):
                name = match.group('name').lower()
                try:
                    value = float(match.group('value'))
                    # è½¬æ¢ä¸ºæ ‡å‡†åç§°
                    std_name = self.METRIC_ALIASES.get(name, name)
                    result.config[std_name] = value
                except (ValueError, TypeError):
                    pass
    
    def _extract_epoch(self, line: str, result: ParsedMetrics) -> None:
        """æå– epoch ä¿¡æ¯"""
        # Epoch X/Y æ ¼å¼
        match = re.search(r'[Ee]poch[\s]*(?:[:=][\s]*)?(\d+)(?:/(\d+))?', line)
        if match:
            epoch = int(match.group(1))
            result.epochs_detected = max(result.epochs_detected, epoch)
    
    def _extract_metrics(self, line: str, result: ParsedMetrics) -> None:
        """æå–æŒ‡æ ‡å€¼"""
        # æ–¹æ³•1ï¼šç›´æ¥åŒ¹é… key: value æˆ– key=value æ ¼å¼
        # æ”¯æŒå¤šç§æ ¼å¼
        patterns = [
            # loss: 0.1234 æˆ– loss=0.1234
            r'(?P<name>\b(?:train_?|val_?|test_?|validation_?)?(?:loss|acc(?:uracy)?|f1(?:_?score)?|precision|recall|auc|mse|mae|rmse)\b)[\s]*[:=][\s]*(?P<value>[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?)[\s]*%?',
            # Loss: 0.1234ï¼ˆé¦–å­—æ¯å¤§å†™ï¼‰
            r'(?P<name>\b(?:Train_?|Val_?|Test_?|Validation_?)?(?:Loss|Acc(?:uracy)?|F1(?:_?[Ss]core)?|Precision|Recall|AUC|MSE|MAE|RMSE)\b)[\s]*[:=][\s]*(?P<value>[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?)[\s]*%?',
        ]
        
        for pattern in patterns:
            for match in re.finditer(pattern, line, re.IGNORECASE):
                name = match.group('name').lower().strip()
                value_str = match.group('value')
                
                try:
                    value = float(value_str)
                    
                    # å¦‚æœæ˜¯ç™¾åˆ†æ¯”æ ¼å¼ï¼ˆè¡Œä¸­æœ‰ %ï¼‰ï¼Œè½¬æ¢ä¸ºå°æ•°
                    if '%' in line[match.end():match.end()+2]:
                        value = value / 100.0
                    
                    # æ ‡å‡†åŒ–åç§°
                    std_name = self._standardize_metric_name(name)
                    
                    # æ·»åŠ åˆ°å†å²
                    if std_name not in result.history:
                        result.history[std_name] = []
                    result.history[std_name].append(value)
                    
                except (ValueError, TypeError):
                    pass
    
    def _extract_best_metrics(self, line: str, result: ParsedMetrics) -> None:
        """æå–æœ€ä½³æŒ‡æ ‡ï¼ˆä» "best" æˆ– "æœ€ä½³" ç›¸å…³è¡Œï¼‰"""
        lower_line = line.lower()
        if 'best' in lower_line or 'æœ€ä½³' in line or 'ä¿å­˜' in line:
            # å°è¯•æå–æŒ‡æ ‡å€¼
            # æ ¼å¼: "Best accuracy: 0.9782" æˆ– "æœ€ä½³å‡†ç¡®ç‡: 97.82%"
            match = re.search(
                r'(?:best|æœ€ä½³)[\s]*(?:[\w_]*)?[\s]*[:=]?[\s]*(?P<value>[-+]?\d*\.?\d+)[\s]*%?',
                line, re.IGNORECASE
            )
            if match:
                try:
                    value = float(match.group('value'))
                    # æ£€æµ‹æ˜¯ç™¾åˆ†æ¯”è¿˜æ˜¯å°æ•°
                    if '%' in line or value > 1.5:  # å¤§äº1.5é€šå¸¸æ˜¯ç™¾åˆ†æ¯”
                        value = value / 100.0
                    result.best_metrics['best_accuracy'] = value
                except (ValueError, TypeError):
                    pass
    
    def _standardize_metric_name(self, name: str) -> str:
        """æ ‡å‡†åŒ–æŒ‡æ ‡åç§°"""
        name = name.lower().strip()
        name = name.replace('-', '_').replace(' ', '_')
        
        # ä½¿ç”¨åˆ«åæ˜ å°„
        if name in self.METRIC_ALIASES:
            return self.METRIC_ALIASES[name]
        
        return name
    
    def _finalize_metrics(self, result: ParsedMetrics) -> None:
        """ç”Ÿæˆæœ€ç»ˆæŒ‡æ ‡"""
        # å¯¹äºæ¯ä¸ªæŒ‡æ ‡ï¼Œå–æœ€åä¸€ä¸ªå€¼ä½œä¸ºæœ€ç»ˆå€¼
        for name, values in result.history.items():
            if values:
                result.final_metrics[f'final_{name}'] = values[-1]
                
                # å¦‚æœæœ‰å¤šä¸ªå€¼ï¼Œä¹Ÿè®°å½•ç¬¬ä¸€ä¸ªå€¼ç”¨äºå¯¹æ¯”
                if len(values) > 1:
                    result.final_metrics[f'initial_{name}'] = values[0]
        
        # æ·»åŠ æœ€ä½³æŒ‡æ ‡
        result.final_metrics.update(result.best_metrics)
        
        # æ·»åŠ  epoch æ•°
        if result.epochs_detected > 0:
            result.final_metrics['epochs_completed'] = result.epochs_detected


def parse_log_file(log_path: str) -> Dict[str, float]:
    """
    è§£ææ—¥å¿—æ–‡ä»¶ï¼Œè¿”å›æå–çš„æŒ‡æ ‡
    
    Args:
        log_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        
    Returns:
        æŒ‡æ ‡å­—å…¸
    """
    try:
        with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        parser = LogParser()
        result = parser.parse(content)
        
        return result.final_metrics
    except Exception as e:
        # é™é»˜å¤±è´¥ï¼Œè¿”å›ç©ºå­—å…¸
        return {}


def parse_log_content(content: str) -> Dict[str, float]:
    """
    è§£ææ—¥å¿—å†…å®¹å­—ç¬¦ä¸²ï¼Œè¿”å›æå–çš„æŒ‡æ ‡
    
    Args:
        content: æ—¥å¿—æ–‡æœ¬å†…å®¹
        
    Returns:
        æŒ‡æ ‡å­—å…¸
    """
    parser = LogParser()
    result = parser.parse(content)
    return result.final_metrics


# ä¾¿æ·å‡½æ•°
def extract_metrics_from_stdout(stdout_content: str) -> Dict[str, float]:
    """
    ä» stdout è¾“å‡ºä¸­æå–æŒ‡æ ‡ï¼ˆguardian.py è°ƒç”¨æ­¤å‡½æ•°ï¼‰
    
    Args:
        stdout_content: stdout è¾“å‡ºå†…å®¹
        
    Returns:
        æå–çš„æŒ‡æ ‡å­—å…¸
    """
    return parse_log_content(stdout_content)


if __name__ == '__main__':
    # æµ‹è¯•ç”¨ä¾‹
    test_logs = [
        # PyTorch é£æ ¼
        """
        Epoch 1/10
        Train Loss: 0.5234, Train Acc: 82.34%
        Val Loss: 0.4123, Val Acc: 87.56%
        
        Epoch 2/10
        Train Loss: 0.3456, Train Acc: 89.12%
        Val Loss: 0.3012, Val Acc: 91.23%
        
        Best accuracy: 91.23%
        """,
        
        # TensorFlow/Keras é£æ ¼
        """
        Epoch 1/5
        loss: 0.6931 - accuracy: 0.5234 - val_loss: 0.6543 - val_accuracy: 0.6789
        Epoch 2/5
        loss: 0.4567 - accuracy: 0.7890 - val_loss: 0.4321 - val_accuracy: 0.8234
        """,
        
        # ç®€å•æ ¼å¼
        """
        Training...
        loss=0.234, acc=0.956
        Final loss: 0.123, Final acc: 0.978
        """,
        
        # ä¸­æ–‡æ ¼å¼
        """
        ğŸ“ˆ Epoch 1/5 (lr=0.001000)
           è®­ç»ƒ - Loss: 0.2345, Acc: 92.34%
           æµ‹è¯• - Loss: 0.1234, Acc: 97.82%
           âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (Acc: 97.82%)
        """,
    ]
    
    print("=" * 60)
    print("LogParser æµ‹è¯•")
    print("=" * 60)
    
    for i, log in enumerate(test_logs, 1):
        print(f"\n--- æµ‹è¯•ç”¨ä¾‹ {i} ---")
        metrics = parse_log_content(log)
        print(f"æå–çš„æŒ‡æ ‡: {metrics}")

