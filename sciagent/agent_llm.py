import os 
from typing import Literal, Optional, Iterator
from openai import OpenAI
from dotenv import load_dotenv

SUPPORTED_PROVIDERS=Literal[
    "openai", "deepseek", "qwen", "modelscope",
    "kimi", "zhipu", "gemini", "claude", "vllm", "local", "auto"
]

class AgentsLLM:
    
    def __init__(
        self,
        model: Optional[str] = None,
        api_key:Optional[str] = None,
        base_url: Optional[str] = None,
        provider: Optional[SUPPORTED_PROVIDERS] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        timeout: Optional[int] = None,
        **kwargs
    ):

        self.model = model or os.getenv("LLM_MODEL_ID")
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        self.kwargs = kwargs

        self.provider = provider or self._auto_detect_provider(api_key, base_url)
        self.api_key, self.base_url = self._resolve_credentials(api_key, base_url)

         # éªŒè¯å¿…è¦å‚æ•°
        if not self.model:
            self.model= self._get_default_model()
        if not all([self.api_key, self.base_url]):
            raise Exception("APIå¯†é’¥å’ŒæœåŠ¡åœ°å€å¿…é¡»è¢«æä¾›æˆ–åœ¨.envæ–‡ä»¶ä¸­å®šä¹‰")

         #åˆ›å»ºOpenAIå®¢æˆ·ç«¯
        self._client = self._create_client()



    def _create_client(self) ->OpenAI:

        return OpenAI(
            api_key = self.api_key,
            base_url = self.base_url,
            timeout = self.timeout
        )

    def _auto_detect_provider(self, api_key:Optional[str], base_url:Optional[str]) ->str:

        if os.getenv("OPENAI_API_KEY"):
            return "openai"
        if os.getenv("DEEPSEEK_API_KEY"):
            return "deepseek"
        if os.getenv("DASHSCOPE_API_KEY"):
            return "qwen"
        if os.getenv("MODELSCOPE_API_KEY"):
            return "modelscope"
        if os.getenv("KIMI_API_KEY") or os.getenv("MOONSHOT_API_KEY"):
            return "kimi"
        if os.getenv("ZHIPU_API_KEY") or os.getenv("GLM_API_KEY"):
            return "zhipu"
        if os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY"):
            return "gemini"
        if os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY"):
            return "claude"
        if os.getenv("VLLM_API_KEY") or os.getenv("VLLM_HOST"):
            return "vllm"

        # 2. æ ¹æ®APIå¯†é’¥æ ¼å¼åˆ¤æ–­
        actual_api_key = api_key or os.getenv("LLM_API_KEY")
        if actual_api_key:
            actual_key_lower = actual_api_key.lower()
            if actual_api_key.startswith("ms-"):
                return "modelscope"
            elif actual_api_key.startswith("AIza"):
                # Gemini API Key æ ¼å¼
                return "gemini"
            elif actual_api_key.startswith("sk-ant-"):
                # Claude API Key æ ¼å¼
                return "claude"
            elif actual_key_lower == "vllm":
                return "vllm"
            elif actual_key_lower == "local":
                return "local"
            elif actual_api_key.startswith("sk-") and len(actual_api_key) > 50:
                # å¯èƒ½æ˜¯OpenAIã€DeepSeekæˆ–Kimiï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ¤æ–­
                pass
            elif actual_api_key.endswith(".") or "." in actual_api_key[-20:]:
                # æ™ºè°±AIçš„APIå¯†é’¥æ ¼å¼é€šå¸¸åŒ…å«ç‚¹å·
                return "zhipu"

        # 3. æ ¹æ®base_urlåˆ¤æ–­
        actual_base_url = base_url or os.getenv("LLM_BASE_URL")
        if actual_base_url:
            base_url_lower = actual_base_url.lower()
            if "api.openai.com" in base_url_lower:
                return "openai"
            elif "api.deepseek.com" in base_url_lower:
                return "deepseek"
            elif "dashscope.aliyuncs.com" in base_url_lower:
                return "qwen"
            elif "api-inference.modelscope.cn" in base_url_lower:
                return "modelscope"
            elif "api.moonshot.cn" in base_url_lower:
                return "kimi"
            elif "open.bigmodel.cn" in base_url_lower:
                return "zhipu"
            elif "generativelanguage.googleapis.com" in base_url_lower or "ai.google.dev" in base_url_lower:
                return "gemini"
            elif "api.anthropic.com" in base_url_lower:
                return "claude"
            elif "localhost" in base_url_lower or "127.0.0.1" in base_url_lower:
                # æœ¬åœ°éƒ¨ç½²æ£€æµ‹ - ä¼˜å…ˆæ£€æŸ¥ç‰¹å®šæœåŠ¡
                if ":8000" in base_url_lower and "vllm" in base_url_lower:
                    return "vllm"
                elif ":8080" in base_url_lower or ":7860" in base_url_lower:
                    return "local"
                else:
                    # æ ¹æ®APIå¯†é’¥è¿›ä¸€æ­¥åˆ¤æ–­
                    if actual_api_key and actual_api_key.lower() == "vllm":
                        return "vllm"
                    else:
                        return "local"
            elif any(port in base_url_lower for port in [":8080", ":7860", ":5000"]):
                # å¸¸è§çš„æœ¬åœ°éƒ¨ç½²ç«¯å£
                return "local"

        # 4. é»˜è®¤è¿”å›autoï¼Œä½¿ç”¨é€šç”¨é…ç½®
        return "auto"

    def _resolve_credentials(self, api_key: Optional[str], base_url: Optional[str]) -> tuple[str, str]:
        """æ ¹æ®providerè§£æAPIå¯†é’¥å’Œbase_url"""
        if self.provider == "openai":
            resolved_api_key = api_key or os.getenv("OPENAI_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL") or "https://api.openai.com/v1"
            return resolved_api_key, resolved_base_url

        elif self.provider == "deepseek":
            resolved_api_key = api_key or os.getenv("DEEPSEEK_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL") or "https://api.deepseek.com"
            return resolved_api_key, resolved_base_url

        elif self.provider == "qwen":
            resolved_api_key = api_key or os.getenv("DASHSCOPE_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL") or "https://dashscope.aliyuncs.com/compatible-mode/v1"
            return resolved_api_key, resolved_base_url

        elif self.provider == "modelscope":
            resolved_api_key = api_key or os.getenv("MODELSCOPE_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL") or "https://api-inference.modelscope.cn/v1/"
            return resolved_api_key, resolved_base_url

        elif self.provider == "kimi":
            resolved_api_key = api_key or os.getenv("KIMI_API_KEY") or os.getenv("MOONSHOT_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL") or "https://api.moonshot.cn/v1"
            return resolved_api_key, resolved_base_url

        elif self.provider == "zhipu":
            resolved_api_key = api_key or os.getenv("ZHIPU_API_KEY") or os.getenv("GLM_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL") or "https://open.bigmodel.cn/api/paas/v4"
            return resolved_api_key, resolved_base_url

        elif self.provider == "gemini":
            resolved_api_key = api_key or os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL") or "https://generativelanguage.googleapis.com/v1beta"
            return resolved_api_key, resolved_base_url

        elif self.provider == "claude":
            resolved_api_key = api_key or os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL") or "https://api.anthropic.com/v1"
            return resolved_api_key, resolved_base_url

        elif self.provider == "vllm":
            resolved_api_key = api_key or os.getenv("VLLM_API_KEY") or os.getenv("LLM_API_KEY") or "vllm"
            resolved_base_url = base_url or os.getenv("VLLM_HOST") or os.getenv("LLM_BASE_URL") or "http://localhost:8000/v1"
            return resolved_api_key, resolved_base_url

        elif self.provider == "local":
            resolved_api_key = api_key or os.getenv("LLM_API_KEY") or "local"
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL") or "http://localhost:8000/v1"
            return resolved_api_key, resolved_base_url

        else:
            # autoæˆ–å…¶ä»–æƒ…å†µï¼šä½¿ç”¨é€šç”¨é…ç½®ï¼Œæ”¯æŒä»»ä½•OpenAIå…¼å®¹çš„æœåŠ¡
            resolved_api_key = api_key or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL")
            return resolved_api_key, resolved_base_url

    def _get_default_model(self) -> str:
        if self.provider == "openai":
            return "gpt-5.1"
        elif self.provider == "deepseek":
            return "deepseek-chat"
        elif self.provider == "qwen":
            return "qwen-plus"
        elif self.provider == "modelscope":
            return "Qwen/Qwen2.5-72B-Instruct"
        elif self.provider == "kimi":
            return "moonshot-v1-8k"
        elif self.provider == "zhipu":
            return "glm-4.6"
        elif self.provider == "gemini":
            return "gemini-2.5-flash"
        elif self.provider == "claude":
            return "claude-sonnet-4-5"
        elif self.provider == "vllm":
            return "meta-llama/Llama-2-7b-chat-hf"  # vLLMå¸¸ç”¨æ¨¡å‹
        elif self.provider == "local":
            return "local-model"  # æœ¬åœ°æ¨¡å‹å ä½ç¬¦
        else:
            # autoæˆ–å…¶ä»–æƒ…å†µï¼šæ ¹æ®base_urlæ™ºèƒ½æ¨æ–­é»˜è®¤æ¨¡å‹
            base_url = os.getenv("LLM_BASE_URL", "")
            base_url_lower = base_url.lower()
            if "modelscope" in base_url_lower:
                return "Qwen/Qwen2.5-72B-Instruct"
            elif "deepseek" in base_url_lower:
                return "deepseek-chat"
            elif "dashscope" in base_url_lower:
                return "qwen-plus"
            elif "moonshot" in base_url_lower:
                return "moonshot-v1-8k"
            elif "bigmodel" in base_url_lower:
                return "glm-4.6"
            elif "googleapis.com" in base_url_lower or "ai.google" in base_url_lower:
                return "gemini-2.5-flash"
            elif "anthropic" in base_url_lower:
                return "claude-sonnet-4-5"
            elif ":8000" in base_url_lower or "vllm" in base_url_lower:
                return "meta-llama/Llama-2-7b-chat-hf"
            elif "localhost" in base_url_lower or "127.0.0.1" in base_url_lower:
                return "local-model"
            else:
                return "gpt-5.1"     

    def think(self, messages: list[dict[str,str]], temperature:Optional[float] = None) ->Iterator[str]:
        """
        æµå¼è°ƒç”¨
        """

        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...")
        try:
            response = self._client.chat.completions.create(
                model=self.model,
                messages = messages,
                temperature = temperature if temperature is not None else self.temperature,
                max_tokens = self.max_tokens,
                stream = True,
            )

            print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:")
            for chunk in response:
                content = chunk.choices[0].delta.content or ""
                if content:
                    print(content, end="", flush=True)
                    yield content
                
            print()
        except Exception as e:
            print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")

    def invoke(self, messages: list[dict[str, str]], **kwargs) -> str:
        """
        éæµå¼è°ƒç”¨
        """
        pass



if __name__ == "__main__":
    load_dotenv()
    test_agent = AgentsLLM()
    print(f"ä½¿ç”¨çš„å¤§æ¨¡å‹æœåŠ¡å•†ä¸ºï¼š{test_agent.provider}")
    print(f"ä½¿ç”¨çš„å¤§æ¨¡å‹ä¸ºï¼š{test_agent.model}")
    print(f"urlåœ°å€ä¸º:{test_agent.base_url}")

        
    Messages = [
        {"role": "system", "content": "You are a helpful assistant that writes Python code."},
        {"role": "user", "content": "å†™ä¸€ä¸ªå¹¶æŸ¥é›†ç®—æ³•"}
    ]
    
    print("--- è°ƒç”¨LLM ---")
    responseText = test_agent.think(Messages)
    if responseText:
        print("\n\n--- å®Œæ•´æ¨¡å‹å“åº” ---")
        for chunk in responseText:
            print(chunk, end="", flush=True) # å®æ—¶æ‰“å°æ¯ä¸ªæ–‡æœ¬å—
        print() # æ‰“å°å®Œæˆåæ¢è¡Œ


